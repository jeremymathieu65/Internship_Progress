{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Basics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNF53oXw0P8vITOD3fxW+zv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"XANflw96eLfE","executionInfo":{"status":"ok","timestamp":1627496165686,"user_tz":-300,"elapsed":5060,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["import torch\n","import torch.nn as nn\n","\n","import pprint\n","pp = pprint.PrettyPrinter()"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xr28hpKeuXT"},"source":["# Using Tensors in PyTorch\n","\n","# Initializing tensors\n","Below are demonstrations of the different ways tensors can be initialized in PyTorch:\n","### 1 - From a list\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPtZEOe_fii6","executionInfo":{"status":"ok","timestamp":1627496165687,"user_tz":-300,"elapsed":94,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"031d778f-ab57-41cf-e292-3dda759fef17"},"source":["arr = [range(10)]\n","arr_t = torch.tensor(arr)\n","arr_t\n","\n","# Typecasting tensors\n","# Way 1: use dtype argument in torch.tensor()\n","# Important dtypes = torch.float, torch.bool, torch.long\n","arr_t_float = torch.tensor(arr, dtype = torch.float)\n","arr_t_float\n","# Way 2: use <tensor name>.<data type>() method of an existing tensor\n","arr_t.float()\n","# Way 3: using the torch.Tensor() function (with capital T), which by default instantiates a tensor of type float.\n","# torch.FloatTensor() can be used to achieve same functionality as torch.Tensor()\n","# torch.LongTensor() can be used to initialize a tensor of type 64-bit int\n","arr_t_long = torch.LongTensor(arr)\n","arr_t_long\n","arr_t_long.shape"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 10])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"QA7wdzcRiLyl"},"source":["### 2 - From a NumPy array"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0WY6juUiQeq","executionInfo":{"status":"ok","timestamp":1627496165687,"user_tz":-300,"elapsed":89,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"a815f7bf-9c72-4baa-9ea9-de2a94f56ba2"},"source":["import numpy as np\n","\n","# Can convert a numpy array to a tensor using the torch.from_numpy() function\n","arr_np = np.array(arr)\n","arr_np\n","arr_t_np = torch.from_numpy(arr_np)\n","arr_t_np"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"L_KukxRoiv1x"},"source":["### 3 - From an existing tensor"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GP_Qw5Hfiyv8","executionInfo":{"status":"ok","timestamp":1627496165687,"user_tz":-300,"elapsed":86,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"489b9a80-e150-44bf-d0be-0f58a73144fe"},"source":["# Can use 1 of 4 functions to initialize a tensor from an exisitng tensor\n","# The torch.zeros_like(<Existing Tensor>) function initializes a tensor of 0s with the same shape and device as the existing tensor\n","t_zeros = torch.zeros_like(arr_t)\n","t_zeros"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhwyzL6bjHet","executionInfo":{"status":"ok","timestamp":1627496165688,"user_tz":-300,"elapsed":85,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"5d0a8c5b-2a05-439c-c100-f6be451c266f"},"source":["# The torch.ones_like(<Existing Tensor>) function initializes a tensor of 1s with the same shape and device as the existing tensor\n","t_ones = torch.ones_like(arr_t)\n","t_ones"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeDfFv9OjQGJ","executionInfo":{"status":"ok","timestamp":1627496165688,"user_tz":-300,"elapsed":83,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"a4926af0-61ea-4622-ccb6-b77a8243df58"},"source":["# The torch.rand_like(<Existing Tensor>) function initializes a tensor with values sampled from a uniform distribution bounded by 0 and 1 having the same shape and device as the existing tensor\n","t_rand = torch.rand_like(arr_t.float())\n","t_rand"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.2834, 0.9870, 0.0276, 0.2358, 0.9876, 0.3671, 0.1337, 0.7062, 0.9826,\n","         0.8990]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMKoqDfbkECq","executionInfo":{"status":"ok","timestamp":1627496165688,"user_tz":-300,"elapsed":80,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"007fa02b-ac84-48a9-dd8b-30194c98edd9"},"source":["# The torch.randn_like(<Existing Tensor>) function initializes a tensor with values sampled from the normal distribution having the same shape and device as the existing tensor\n","t_randn = torch.randn_like(arr_t.float())\n","t_randn\n","\n","# Important note: torch.rand_like() and torch.randn_like() work only for existing tensors of type float"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.0621, -1.2848, -0.8471, -0.3908,  1.6612,  1.4181,  0.1920,  0.2399,\n","          0.7167, -0.2970]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"gOyMZdG0k8kF"},"source":["### 4 - By specifiying tensor shape (dimensions)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZHJZAt9lBeQ","executionInfo":{"status":"ok","timestamp":1627496165689,"user_tz":-300,"elapsed":78,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"c731d434-1afb-4314-d55c-bc6d2b3e89c0"},"source":["# Can use 1 of 4 functions, each having names similar to the 4 in the above section, with the exception of the 'like' suffix\n","# torch.zeros(<shape>) can be used to initialize a tensor of 0s of dimensions specified shape argument\n","shape = (4, 3, 2)\n","torch.zeros(shape)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0.],\n","         [0., 0.],\n","         [0., 0.]],\n","\n","        [[0., 0.],\n","         [0., 0.],\n","         [0., 0.]],\n","\n","        [[0., 0.],\n","         [0., 0.],\n","         [0., 0.]],\n","\n","        [[0., 0.],\n","         [0., 0.],\n","         [0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJr7v_BZl9z3","executionInfo":{"status":"ok","timestamp":1627496165689,"user_tz":-300,"elapsed":76,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"d7513ac6-5968-433f-bf03-91c59f484c42"},"source":["# torch.ones() can be used to initialize tensor of 1s\n","torch.ones(shape)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1.],\n","         [1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.],\n","         [1., 1.]]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgDO5VPmmEOc","executionInfo":{"status":"ok","timestamp":1627496165689,"user_tz":-300,"elapsed":74,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"3e0a2625-525e-447a-f971-c1697d2feeed"},"source":["# torch.rand() can be used to initialize tensor of values sampled from uniform distirbution between 0 and 1\n","torch.rand(shape)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.6988, 0.4774],\n","         [0.6486, 0.8024],\n","         [0.4054, 0.3805]],\n","\n","        [[0.1979, 0.1019],\n","         [0.1972, 0.2979],\n","         [0.3218, 0.7185]],\n","\n","        [[0.7333, 0.3482],\n","         [0.2904, 0.8147],\n","         [0.9556, 0.6868]],\n","\n","        [[0.5627, 0.4832],\n","         [0.2455, 0.9455],\n","         [0.9203, 0.0527]]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yy0XDmu2mOQA","executionInfo":{"status":"ok","timestamp":1627496165690,"user_tz":-300,"elapsed":73,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"09f99264-58dd-4d98-abe4-778b9d71bea3"},"source":["# torch.randn() can be used to initialize tensor of values sampled from the normal distribution\n","torch.randn(shape)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.3745, -0.7559],\n","         [ 1.0815, -0.0441],\n","         [ 0.1290,  0.5564]],\n","\n","        [[ 0.1326,  2.1671],\n","         [ 0.9182,  0.6388],\n","         [ 0.1839,  0.5316]],\n","\n","        [[ 0.9678,  1.1021],\n","         [-1.6087,  0.7781],\n","         [ 0.9970, -2.2567]],\n","\n","        [[ 0.2437, -0.5370],\n","         [-1.3689,  0.7581],\n","         [-0.6686, -1.4012]]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"NnLWIz7Eokmy"},"source":["### 5 - Using torch.arange()"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"az5qV2GnouJ1","executionInfo":{"status":"ok","timestamp":1627496165690,"user_tz":-300,"elapsed":71,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"90e13859-2647-47f7-d00f-9a63ab6d8c2c"},"source":["# Similar to how we use the range() function to initialize a list, can use 'start' and 'step' arguments to specify list values\n","# Using torch.arange() without start and step args\n","torch.arange(10)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuuHK09KtUrn","executionInfo":{"status":"ok","timestamp":1627496165690,"user_tz":-300,"elapsed":68,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"187578e2-590d-49fd-a245-56ab6a4ce728"},"source":["# Using torch.arange() with start and end arg\n","torch.arange(start = 5, end = 10)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 6, 7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VE9sHe_vnGK","executionInfo":{"status":"ok","timestamp":1627496165691,"user_tz":-300,"elapsed":66,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"bb1b8bb3-ff37-4f37-b6be-4188e108dc50"},"source":["# Using torch.arange() with step arg\n","torch.arange(5, 10, step=2)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 7, 9])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"K5aBhtbr7jeF"},"source":["### Reshaping Tensors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDiYEIpZ7989","executionInfo":{"status":"ok","timestamp":1627496165691,"user_tz":-300,"elapsed":64,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"4011b067-90f9-4dd2-9d58-46a1aff8acae"},"source":["# We can reshape tensors using the view() method. View takes in a set of dimensions the product of whom must be equal to the product of the dimensions of the current vector\n","# We can ask view() to infer a dimension by specifying a -1 character in the tensor\n","data = [range(10)]\n","data_t = torch.tensor(data)\n","data_t.view(5, -1)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOq9J0GL-dgI","executionInfo":{"status":"ok","timestamp":1627496165691,"user_tz":-300,"elapsed":62,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"1595dcfa-775c-47bf-f8d8-8013b5c32ba1"},"source":["# Another way to reshape vectors is using the reshape() method\n","# Reshape also takes a set of dimensions as input\n","# unsqueeze() method is used to add a dimension that has only 1 element in it and takes an index as argument which defines the index at which the new dimension is to be added\n","# squeeze() method is used to eliminate a dimension that has only 1 element in it\n","data_reshape = data_t.reshape(2, 5)\n","data_unsqueeze = data_reshape.unsqueeze(1)\n","data_squeeze = data_unsqueeze.squeeze(1)\n","data_squeeze"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"9ytM2CFZCBLQ"},"source":["### Extracting Tensor information"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGvsVVTVCGj_","executionInfo":{"status":"ok","timestamp":1627496165691,"user_tz":-300,"elapsed":60,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"8f08d325-5c6e-4bc6-ebcf-9685a2caf942"},"source":["# Extracting the data type\n","data_t.dtype"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.int64"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYKadhaGDFic","executionInfo":{"status":"ok","timestamp":1627496165692,"user_tz":-300,"elapsed":59,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"90bbec8b-0a94-4dbd-bdbb-fa5b74658e2b"},"source":["# Extracting the dimensions\n","data_t.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 10])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X588H4jnDJ5K","executionInfo":{"status":"ok","timestamp":1627496165692,"user_tz":-300,"elapsed":56,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"cc225ac2-8f16-402b-a87f-90a520944868"},"source":["# Extracting the number of elements\n","data_t.numel()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gx_HRGAODNhK","executionInfo":{"status":"ok","timestamp":1627496165692,"user_tz":-300,"elapsed":54,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"7e5f482b-3935-4a6d-d913-62308ea64213"},"source":["# Extracting the number of dimensions\n","data_t.ndim"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"x8hR3112Du9G","executionInfo":{"status":"ok","timestamp":1627496165692,"user_tz":-300,"elapsed":51,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# Extracting where the tensor in stored, cpu or gpu\n","data_t.device\n","\n","# To move a tensor to the GPU\n","if torch.cuda.is_available():\n","  data_t.to('cuda')"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xvQ3P4_HIn5"},"source":["### Indexing a tensor"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkA4E4tiHKii","executionInfo":{"status":"ok","timestamp":1627496165693,"user_tz":-300,"elapsed":50,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"74eacb09-c887-4c46-fdd3-075a65be4b92"},"source":["# The symbol ':' is used to say that all the elements of the specified dimension are to be selected\n","# Indexing can also be done using lists\n","new_tensor = torch.tensor([range(20)])\n","new_tensor = new_tensor.view(2, 5, 2)\n","new_tensor"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  1],\n","         [ 2,  3],\n","         [ 4,  5],\n","         [ 6,  7],\n","         [ 8,  9]],\n","\n","        [[10, 11],\n","         [12, 13],\n","         [14, 15],\n","         [16, 17],\n","         [18, 19]]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmB40hWiJGlT","executionInfo":{"status":"ok","timestamp":1627496165693,"user_tz":-300,"elapsed":48,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"b640a15a-e97a-463a-87d9-b1165abd5dbb"},"source":["# The following code takes the 0th row of the tensor, the 1st, 2nd, and 3rd columns of that row, and the 1st elements of the individual lists at each column\n","i = 0\n","j = [1, 2, 3]\n","k = 1\n","new_tensor[i, j, k]\n","# Example of using ':'\n","# The following code will take all rows and access the lists at the 1st, 2nd, and 3rd columns of each row, and the 1st element of each individual list\n","new_tensor[:, j, k]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 3,  5,  7],\n","        [13, 15, 17]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BSAjX9vKEOz","executionInfo":{"status":"ok","timestamp":1627496165693,"user_tz":-300,"elapsed":45,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"9183c945-8038-4034-ec57-2800d8bf7949"},"source":["# To convert a tensor element to a scalar, use the item attribute\n","new_tensor[1, 2, 1].item()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"oVcgtM1gK1B-"},"source":["### Performing arithmetic operations on tensors\n","Tensors work very similarly to matrices when performing arithmetic operations. Addition, subtraction, multiplication, divison is performed element wise when performing them with a single number"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dm69r1G4Lg7x","executionInfo":{"status":"ok","timestamp":1627496165694,"user_tz":-300,"elapsed":44,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"90dab656-3dff-4778-c7d9-2e2164ccac61"},"source":["new_tensor = torch.tensor(range(10))\n","# Performing addition\n","new_tensor + 5"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIyQdg-9MQsA","executionInfo":{"status":"ok","timestamp":1627496165694,"user_tz":-300,"elapsed":42,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"ffd5086e-798f-44d9-965e-cb7f4057fd8d"},"source":["# Performing subtraction\n","new_tensor - 3"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7LhFNHnMS_1","executionInfo":{"status":"ok","timestamp":1627496165694,"user_tz":-300,"elapsed":39,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"5daf5aeb-d71c-445a-a69a-8a75a9b8ada9"},"source":["# Performing multiplication\n","new_tensor * 5"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  5, 10, 15, 20, 25, 30, 35, 40, 45])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qwpCSlMMVhe","executionInfo":{"status":"ok","timestamp":1627496165695,"user_tz":-300,"elapsed":37,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"ab901c38-df53-400d-ce2e-26d31b6030e7"},"source":["# Performing division\n","(new_tensor / 5)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000, 1.0000, 1.2000, 1.4000, 1.6000,\n","        1.8000])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"2gnGTtWFMhM2"},"source":["Operations between two tensors is performed similarly to how it is performed between two matrices. Specifically, matrix multiplication is performed in the following way: \n","\n","Given two tensors of the following dimensions:\n","\n","rows1 x columns1\n","\n","rows2 x columns2\n","\n","columns1 and rows2 need to be equivalent if the tensors are to be multiplied and the resulting tensor will have dimensions rows1 * columns2"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLUUTzxwNL0V","executionInfo":{"status":"ok","timestamp":1627496165695,"user_tz":-300,"elapsed":35,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"1d1cd4b3-1b35-4562-abe8-506a5187aaae"},"source":["# Initializing a tensor of dimensions 3 x 2\n","tensor_1 = torch.tensor(range(6)).view(3, 2)\n","tensor_1"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIWWmKHlN5js","executionInfo":{"status":"ok","timestamp":1627496165695,"user_tz":-300,"elapsed":33,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"a1a75ee7-464b-4736-edf1-02a5044a7050"},"source":["# Initializing another tensor of dimensions 2 x 4\n","tensor_2 = torch.tensor(range(8)).view(2, 4)\n","tensor_2"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1, 2, 3],\n","        [4, 5, 6, 7]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jlkcuu9N6B4","executionInfo":{"status":"ok","timestamp":1627496165696,"user_tz":-300,"elapsed":32,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"63e93c5a-aa10-4aa1-8fd7-363194aa77dc"},"source":["# Matrix multiplication can be performed using the '@' symbol or using the matmul() method\n","tensor_1 @ tensor_2"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4,  5,  6,  7],\n","        [12, 17, 22, 27],\n","        [20, 29, 38, 47]])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfIT1DyMN6O1","executionInfo":{"status":"ok","timestamp":1627496165696,"user_tz":-300,"elapsed":30,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"4405e012-cd1b-4286-cc9e-65d9c0c0ebb4"},"source":["# Matrix multiplication can be performed using the '@' symbol or using the matmul() method\n","tensor_1.matmul(tensor_2)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4,  5,  6,  7],\n","        [12, 17, 22, 27],\n","        [20, 29, 38, 47]])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"odkPUy8IOUJH","executionInfo":{"status":"ok","timestamp":1627496165696,"user_tz":-300,"elapsed":28,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"48fbbdfe-141e-49ca-d76f-c4a168561d34"},"source":["# To transpose a tensor, use the T attribute\n","tensor_1.T"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 2, 4],\n","        [1, 3, 5]])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEHnFlIlPUoo","executionInfo":{"status":"ok","timestamp":1627496165698,"user_tz":-300,"elapsed":27,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"c3076979-a578-4ebd-9195-cf296f6aabc0"},"source":["tensor_1 = torch.tensor(range(20)).view(2, 5, 2)\n","tensor_1"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  1],\n","         [ 2,  3],\n","         [ 4,  5],\n","         [ 6,  7],\n","         [ 8,  9]],\n","\n","        [[10, 11],\n","         [12, 13],\n","         [14, 15],\n","         [16, 17],\n","         [18, 19]]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ff6wKiknQ9H8","executionInfo":{"status":"ok","timestamp":1627496165698,"user_tz":-300,"elapsed":24,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"46778cfe-6c7e-4b48-85ee-474fe4c5a2ed"},"source":["# The mean across the dimensions of a tensor can be found using the mean() method that takes as parameter the dimension across which the mean is to be commputed\n","# mean() method only works on tensors of dtype float\n","# Mean across the 0th dimension. As the tensor is 2 x 5 x 2, mean across the 0th dimension gives the mean 5 x 2 tensor\n","tensor_1.float().mean(0)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5.,  6.],\n","        [ 7.,  8.],\n","        [ 9., 10.],\n","        [11., 12.],\n","        [13., 14.]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfiBPymqp6pp","executionInfo":{"status":"ok","timestamp":1627496167183,"user_tz":-300,"elapsed":1506,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"ccb43f8d-0c2e-43d9-9e62-a3df9b20fa30"},"source":["# Mean across 1st dimension, provides mean 2 x 2 tensor\n","tensor_1.float().mean(1)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4.,  5.],\n","        [14., 15.]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQaO_k9UqGpH","executionInfo":{"status":"ok","timestamp":1627496167184,"user_tz":-300,"elapsed":94,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"67f66f35-3c87-4693-d66f-4b01a55b04e7"},"source":["# Mean across the 2nd dimension, provides mean 2 x 5 tensor\n","tensor_1.float().mean(2)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5000,  2.5000,  4.5000,  6.5000,  8.5000],\n","        [10.5000, 12.5000, 14.5000, 16.5000, 18.5000]])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHFwaOM0qcW7","executionInfo":{"status":"ok","timestamp":1627496167184,"user_tz":-300,"elapsed":91,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"fc448d4a-d1a7-4a8a-eaca-0e71be37e35a"},"source":["# Standard Deviation can also be calculated across dimensions of tensors of dtype float\n","# Standard deviation across the 0th dimension\n","tensor_1.float().std(0)"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[7.0711, 7.0711],\n","        [7.0711, 7.0711],\n","        [7.0711, 7.0711],\n","        [7.0711, 7.0711],\n","        [7.0711, 7.0711]])"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"FxGugCrFrGLO","executionInfo":{"status":"ok","timestamp":1627496167184,"user_tz":-300,"elapsed":88,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# Concatenation (joining) of 2 or more tensors is done using the torch.cat() function that takes the following arguments:\n","# 1 - A list of tensors to be concatenated\n","# 2 - The dimension across which the concatenation is supposed to happen\n","# For example, if the dim argument is set to 0 (default), the tensors will be concatenated vertically, rows of each individual tensor will be added\n","# If the dim argument is set to 1, the tensors will be concatenated horizontally, columns of each individual tensor will be added\n","tensor_1 = torch.arange(10).view(5, 2)\n","tensor_2 = torch.arange(10, 20).view(5, 2)\n","tensor_cat_0 = torch.cat([tensor_1, tensor_2], dim = 0)\n","tensor_cat_1 = torch.cat([tensor_1, tensor_2], dim = 1)"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mboDqH-dssCs","executionInfo":{"status":"ok","timestamp":1627496167185,"user_tz":-300,"elapsed":88,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"6f2fb5ee-e8da-4246-c2cb-59f928b28949"},"source":["# In-place operations in PyTorch are those that modify the tensor that invokes them and are denoted by an underscore (_) as the suffix to the method name\n","# A normal addition operation (not in-place)\n","tensor_1 + tensor_2"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10, 12],\n","        [14, 16],\n","        [18, 20],\n","        [22, 24],\n","        [26, 28]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zQMTIGRtZw7","executionInfo":{"status":"ok","timestamp":1627496167185,"user_tz":-300,"elapsed":86,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"a7de8e03-003e-4acd-e609-c0d67d4c671b"},"source":["# An In-place operation, tensor_1 stores the result of tensor_1 + tensor_2\n","tensor_1.add_(tensor_2)\n","tensor_1"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10, 12],\n","        [14, 16],\n","        [18, 20],\n","        [22, 24],\n","        [26, 28]])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"z30AJWDCt86p"},"source":["### Calculating gradients"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bGXVX31t_6z","executionInfo":{"status":"ok","timestamp":1627496167185,"user_tz":-300,"elapsed":84,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"0dc063d0-c43c-4ce9-e12d-35ded154c988"},"source":["# PyTorch provides an autograd feature that is used to automatically perform backpropagation and calculate gradients\n","# There are 3 important steps to do this\n","# 1 - Set the requires_grad attribute to True when declaring tensor using torch.tensor()\n","# 2 - Call the backward() method on the tensor that was computed using the tensor that has the requires_grad attribute set to True\n","# 3 - Access the grad attribute of the original tensor to get the gradient\n","# Calculating the gradient of y = x^3 for x = {1, 2, 3, 5}\n","# Derivative of x^3 is 3x^2 so gradients should be {3, 12, 27, 75}\n","xs = torch.tensor([1., 2., 3., 5.], requires_grad=True)\n","grads = []\n","for x in xs:\n","  y = x * x * x\n","  y.backward()\n","xs.grad"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 3., 12., 27., 75.])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JhNuiazv1o9","executionInfo":{"status":"ok","timestamp":1627496167185,"user_tz":-300,"elapsed":80,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"4b76c190-96ac-47ed-d7f7-c34f10b52d3b"},"source":["# One problem of backward() is that it adds up the gradients as we perform subsequent operations\n","# Therefore, the zero_grad() method is to be invoked after each arithmetic operation to prevent gradients from exploding\n","y = xs[0] * xs[0] * xs[0]\n","y.backward()\n","# Actual gradient should be 3, same as above, but computed gradient will be 6 because previous gradient is not discarded\n","xs.grad"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 6., 12., 27., 75.])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"nXgqsHGqz88Z"},"source":["# Building Neural Networks\n","## Training Linear Layers using Tensors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ftAU8560MjO","executionInfo":{"status":"ok","timestamp":1627496167186,"user_tz":-300,"elapsed":78,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"41373d78-c344-4f33-f908-e9c9eb7ec97a"},"source":["# Importing PyTorch's Neural Network module torch.nn with an alias\n","import torch.nn as nn\n","# In order to train a linear layer, we use the nn.Linear() method that takes the following arguments:\n","# 1 - H_in : This represents the number of features per sample (per row) in the input matrix\n","# 2 - H_out : This represents the number of features per sample (per row) of the output matrix\n","# The input matrix will have shape N x * x H_in, where * represents any arbitrary number of dimensions in between\n","# The output matrix will have shape N x * x H_out, where * represents the same arbitrary number of dimensions in the input matrix\n","# The Linear Layer performs the simple operation Ax + b, where:\n","# 1 - A is the learnable weight matrix of shape H_out x H_in \n","# 2 - x is the input matrix of shape N x * x H_in\n","# 3 - b is the learnable bias matrix having a single dimension H_out\n","\n","# Initializing input matrix x\n","x = torch.randn(5, 2, 4)\n","# Initializing Linear Layer with H_in = 4, H_out = 2\n","linear = nn.Linear(4, 2)\n","# Applying Linear Layer to x\n","linear_output = linear(x)\n","# Printing the output matrix of shape 5 x 2 x 2\n","linear_output"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.0082,  0.5932],\n","         [-0.0449, -0.3718]],\n","\n","        [[ 0.3209, -0.3980],\n","         [-0.9270,  0.7147]],\n","\n","        [[ 0.3592, -0.4881],\n","         [-0.3677,  0.4989]],\n","\n","        [[-0.3953,  0.0140],\n","         [ 0.4554, -0.4031]],\n","\n","        [[-0.7613,  0.3426],\n","         [-0.8563,  0.1040]]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QS2VmzGu2lCN","executionInfo":{"status":"ok","timestamp":1627496167186,"user_tz":-300,"elapsed":75,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"39b0371c-06c9-4ade-a54b-a0b8d87af673"},"source":["# Printing the learnable weight matrix A of shape 2 x 4\n","linear.weight"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.1780,  0.4519,  0.0389, -0.3753],\n","        [ 0.3373, -0.4345,  0.0399,  0.0989]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxai9-Mt3Rzb","executionInfo":{"status":"ok","timestamp":1627496167186,"user_tz":-300,"elapsed":73,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"1fc6af67-3564-4349-d1e5-38092673ef09"},"source":["# Printing the learnable bias matrix b of length H_out (2)\n","linear.bias"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([-0.0171, -0.0724], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"TG5gb53b3tu8"},"source":["## Adding Activation Function Layers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iKkHlx4K3yNo","executionInfo":{"status":"ok","timestamp":1627496167187,"user_tz":-300,"elapsed":72,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"4679e856-db11-4cb1-b5d6-abd98791addf"},"source":["# Activation functions are functions that add a non-linearity between 2 linear layers\n","# Examples of activation functions are nn.Sigmoid(), nn.ReLU(), nn.LeakyReLU()\n","# Applying each non-linearity to output matrix of trained linear layer\n","sigmoid = nn.Sigmoid()\n","ReLU = nn.ReLU()\n","LReLU = nn.LeakyReLU()\n","# Printing the output of applying the sigmoid non-linearity\n","sigmoid(linear_output)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4979, 0.6441],\n","         [0.4888, 0.4081]],\n","\n","        [[0.5795, 0.4018],\n","         [0.2835, 0.6714]],\n","\n","        [[0.5888, 0.3803],\n","         [0.4091, 0.6222]],\n","\n","        [[0.4024, 0.5035],\n","         [0.6119, 0.4006]],\n","\n","        [[0.3184, 0.5848],\n","         [0.2981, 0.5260]]], grad_fn=<SigmoidBackward>)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBHzpDO25tts","executionInfo":{"status":"ok","timestamp":1627496167187,"user_tz":-300,"elapsed":70,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"61fca4b8-a341-48a9-94d4-a30d240e4027"},"source":["# Printing the output of applying the ReLU non-linearity\n","ReLU(linear_output)"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0000, 0.5932],\n","         [0.0000, 0.0000]],\n","\n","        [[0.3209, 0.0000],\n","         [0.0000, 0.7147]],\n","\n","        [[0.3592, 0.0000],\n","         [0.0000, 0.4989]],\n","\n","        [[0.0000, 0.0140],\n","         [0.4554, 0.0000]],\n","\n","        [[0.0000, 0.3426],\n","         [0.0000, 0.1040]]], grad_fn=<ReluBackward0>)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnkcWAY55wEx","executionInfo":{"status":"ok","timestamp":1627496167187,"user_tz":-300,"elapsed":66,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"b23af983-10bf-46e0-85f1-1d806171681d"},"source":["# Printing the output of applying the LeakyReLU non-linearity\n","LReLU(linear_output)"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-8.2463e-05,  5.9324e-01],\n","         [-4.4878e-04, -3.7175e-03]],\n","\n","        [[ 3.2092e-01, -3.9800e-03],\n","         [-9.2695e-03,  7.1467e-01]],\n","\n","        [[ 3.5920e-01, -4.8807e-03],\n","         [-3.6769e-03,  4.9894e-01]],\n","\n","        [[-3.9533e-03,  1.4048e-02],\n","         [ 4.5536e-01, -4.0311e-03]],\n","\n","        [[-7.6128e-03,  3.4256e-01],\n","         [-8.5634e-03,  1.0403e-01]]], grad_fn=<LeakyReluBackward0>)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHedXCPMC4tR","executionInfo":{"status":"ok","timestamp":1627496167188,"user_tz":-300,"elapsed":64,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"025c33b6-8ef4-4283-fabd-1ffff54753a2"},"source":["# Combining layers can be done in a handy way through nn.Sequential(). This method takes in all the different layer configruations and applies them\n","# sequentially on the input data. For example, the above code could be re-written as the following to achieve the same output:\n","x = torch.randn(5, 2, 4)\n","block = nn.Sequential(\n","    nn.Linear(4, 2),\n","    nn.Sigmoid()\n",")\n","linear_output = block(x)\n","linear_output"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.5199, 0.8095],\n","         [0.5599, 0.5442]],\n","\n","        [[0.5945, 0.5565],\n","         [0.3706, 0.6706]],\n","\n","        [[0.6647, 0.4774],\n","         [0.6655, 0.4370]],\n","\n","        [[0.6675, 0.4588],\n","         [0.6549, 0.7620]],\n","\n","        [[0.4927, 0.6063],\n","         [0.4508, 0.5421]]], grad_fn=<SigmoidBackward>)"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"qebx3HBIGHUP"},"source":["## Customizing networks using nn.Module\n","### Building a custom network"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TV3P6oHGSNb","executionInfo":{"status":"ok","timestamp":1627496167189,"user_tz":-300,"elapsed":62,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"725e8bf5-ae4c-4602-a18c-84e18eb7c9d3"},"source":["# We can create our own network classes by extending the nn.Module class\n","# 1 - Initialize parameters in the __init__ function of the model class\n","# 2 - Call the __init__ function of the super class (nn.Module) in the __init__ function of the model class\n","class Sample2LayerModel(nn.Module):\n","  def __init__(self, input_size, L1_size, L2_size):\n","    super(Sample2LayerModel, self).__init__()\n","    self.input_size = input_size\n","    self.L1_size = L1_size\n","    self.L2_size = L2_size\n","\n","    self.model = nn.Sequential(\n","        nn.Linear(input_size, L1_size),\n","        nn.ReLU(),\n","        nn.Linear(L1_size, L2_size),\n","        nn.Sigmoid()\n","    )\n","  def forward(self, x):\n","    out_layer = self.model(x)\n","    return out_layer\n","\n","x = torch.randn(2, 4, 5)\n","model = Sample2LayerModel(5, 3, 2)\n","model(x)\n","list(model.parameters())"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.0833,  0.0202,  0.3647,  0.0758,  0.3828],\n","         [ 0.0812,  0.0637,  0.1803, -0.2869,  0.4356],\n","         [-0.0465, -0.2032, -0.3033, -0.3750,  0.1760]], requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.0440, -0.0746,  0.3534], requires_grad=True),\n"," Parameter containing:\n"," tensor([[ 0.3421, -0.4712,  0.2706],\n","         [ 0.1778, -0.4514, -0.0487]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.5738, -0.5398], requires_grad=True)]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"Pm69ZdrkN802"},"source":["### Optimizing the custom-made network"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_sO9jJlOHwB","executionInfo":{"status":"ok","timestamp":1627496167189,"user_tz":-300,"elapsed":60,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"3ca60603-7f75-4a88-fb64-deedf428d059"},"source":["# Model optimization is done using the torch.optim module available in PyTorch\n","# Importing the optim module using an alias\n","import torch.optim as optim\n","\n","# There are many different types of optimizers available in PyTorch, the most popular of which are optim.SGD() and optim.Adam()\n","# Optimizer objects are instantiated by passing the model parameters (via model.parameters()) and the learning rate as arguments\n","# There are 4 important steps to optimizing a model:\n","# 1 - Clear out all the gradients, call zero_grad() method on the optimizer\n","# 2 - Use the model to compute a tensor of predictions for a given input\n","# 3 - Compute the loss of the generated predictions (loss functions pre-built into PyTorch - nn.L1Loss(), nn.CrossEntropyLoss(), nn.MSELoss etc.)\n","# 4 - Perform backpropagation on the loss (call the backward() method)\n","# 5 - Step the weights for the next iteration (call the step() method on the optimizer)\n","\n","y = torch.ones(10, 5)\n","x = y + torch.randn_like(y)\n","model = Sample2LayerModel(5, 3, 5)\n","Adam = optim.Adam(model.parameters(), lr=1e-1)\n","loss_func = nn.BCELoss()\n","for i in range(10):\n","  Adam.zero_grad()\n","  y_pred = model(x)\n","  y_pred\n","  loss = loss_func(y_pred, y)\n","  print(f\"BCE Loss at Epoch # {i}: {loss}\")\n","  loss.backward()\n","  Adam.step()"],"execution_count":52,"outputs":[{"output_type":"stream","text":["BCE Loss at Epoch # 0: 0.6162057518959045\n","BCE Loss at Epoch # 1: 0.4578053653240204\n","BCE Loss at Epoch # 2: 0.29428309202194214\n","BCE Loss at Epoch # 3: 0.16388945281505585\n","BCE Loss at Epoch # 4: 0.07958802580833435\n","BCE Loss at Epoch # 5: 0.03495621308684349\n","BCE Loss at Epoch # 6: 0.01467656996101141\n","BCE Loss at Epoch # 7: 0.006173713598400354\n","BCE Loss at Epoch # 8: 0.002687691478058696\n","BCE Loss at Epoch # 9: 0.0012284605763852596\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_b-HDwgElmQ","executionInfo":{"status":"ok","timestamp":1627496167189,"user_tz":-300,"elapsed":57,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"4cd554ec-79d2-4321-9d33-00bfdda0cd6d"},"source":["x = y + torch.randn_like(y)\n","y_preds = model(x)\n","loss_func(y_preds, y)"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7.5957e-05, grad_fn=<BinaryCrossEntropyBackward>)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"8hRaDzQyK4qV"},"source":["# Building an NLP word classification model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctVfXSG8K-w3","executionInfo":{"status":"ok","timestamp":1627496167189,"user_tz":-300,"elapsed":55,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"2499fcce-906c-49c2-fcaa-f3a05dc7d0dc"},"source":["# In every NLP model, there are 4 main components or stages of development:\n","# 1 - Data - this refers to the creation of dataset of batched word embedding tensors\n","# 2 - Modelling\n","# 3 - Training\n","# 4 - Prediction\n","\n","# Stage 1 - Building a dataset.\n","corpus = [\n","          \"Jack is a boy\",\n","          \"Jill is a girl\",\n","          \"Josephine broke the window\",\n","          \"Joe stood watching\",\n","          \"Andy likes Martha\",\n","          \"Rachel and Ross\",\n","          \"Henry was a beast\",\n","          \"Robert is a cool guy\"\n","]\n","# Defining a list of names to be recognized\n","names = [\"jack\", \"jill\", \"josephine\", \"joe\", \"andy\", \"rachel\", \"henry\", \"robert\", \"ross\"]\n","\n","# Defining a pre-processing function that will lowercase all words and split on whitespace\n","def pre_process(corpus):\n","  return [sentence.lower().split() for sentence in corpus]\n","\n","preprocessed_corpus = pre_process(corpus)\n","preprocessed_corpus"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['jack', 'is', 'a', 'boy'],\n"," ['jill', 'is', 'a', 'girl'],\n"," ['josephine', 'broke', 'the', 'window'],\n"," ['joe', 'stood', 'watching'],\n"," ['andy', 'likes', 'martha'],\n"," ['rachel', 'and', 'ross'],\n"," ['henry', 'was', 'a', 'beast'],\n"," ['robert', 'is', 'a', 'cool', 'guy']]"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPxUlZiZkCz-","executionInfo":{"status":"ok","timestamp":1627496167190,"user_tz":-300,"elapsed":53,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"709b70d3-8d05-4bf6-b46d-15660a0f531c"},"source":["corpus_labels = [[1 if word in names else 0 for word in sentence] for sentence in preprocessed_corpus]\n","corpus_labels"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 0, 0, 0],\n"," [1, 0, 0, 0],\n"," [1, 0, 0, 0],\n"," [1, 0, 0],\n"," [1, 0, 0],\n"," [1, 0, 1],\n"," [1, 0, 0, 0],\n"," [1, 0, 0, 0, 0]]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8R1kt9dLNYC4","executionInfo":{"status":"ok","timestamp":1627496167190,"user_tz":-300,"elapsed":50,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"5898dae2-20fc-4621-f357-a1a1cce92710"},"source":["vocabulary = set(word for sentence in preprocessed_corpus for word in sentence)\n","vocabulary"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a',\n"," 'and',\n"," 'andy',\n"," 'beast',\n"," 'boy',\n"," 'broke',\n"," 'cool',\n"," 'girl',\n"," 'guy',\n"," 'henry',\n"," 'is',\n"," 'jack',\n"," 'jill',\n"," 'joe',\n"," 'josephine',\n"," 'likes',\n"," 'martha',\n"," 'rachel',\n"," 'robert',\n"," 'ross',\n"," 'stood',\n"," 'the',\n"," 'was',\n"," 'watching',\n"," 'window'}"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XPZHHdtOUCU","executionInfo":{"status":"ok","timestamp":1627496167190,"user_tz":-300,"elapsed":47,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"059dc28a-a06d-4953-d938-bc5b3c543afe"},"source":["# Adding a '<pad>' token to vocabulary for later use\n","# Adding a '<unk>' token to vocabulary for later use\n","vocabulary.add(\"<pad>\")\n","vocabulary.add(\"<unk>\")\n","\n","# Creating a sorted index for the vocabulary (word to index correspondance)\n","index = sorted(list(vocabulary))\n","# Creating a index to word correspondance\n","word_index_pair = {word: ind for ind, word in enumerate(index)}\n","word_index_pair"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<pad>': 0,\n"," '<unk>': 1,\n"," 'a': 2,\n"," 'and': 3,\n"," 'andy': 4,\n"," 'beast': 5,\n"," 'boy': 6,\n"," 'broke': 7,\n"," 'cool': 8,\n"," 'girl': 9,\n"," 'guy': 10,\n"," 'henry': 11,\n"," 'is': 12,\n"," 'jack': 13,\n"," 'jill': 14,\n"," 'joe': 15,\n"," 'josephine': 16,\n"," 'likes': 17,\n"," 'martha': 18,\n"," 'rachel': 19,\n"," 'robert': 20,\n"," 'ross': 21,\n"," 'stood': 22,\n"," 'the': 23,\n"," 'was': 24,\n"," 'watching': 25,\n"," 'window': 26}"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lcH8nk_Ptr8","executionInfo":{"status":"ok","timestamp":1627496167190,"user_tz":-300,"elapsed":44,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"9c1f57c7-4c64-4a67-f7d7-e8b2966a2aa4"},"source":["# Defining function to padd a sentence to ensure it fits a given window size\n","def pad_sentence(sentence, window_size, window_token=\"<pad>\"):\n","  window = [window_token] * window_size\n","  return window + sentence + window\n","\n","# Defining a function that will take a tokenized sentence and return a list of the inidividual tokens\n","def token_to_index(sentence, word_to_index):\n","  indices = []\n","  for token in sentence:\n","    if token in word_to_index:\n","      indices.append(word_to_index[token])\n","    else:\n","      indices.append(word_to_index[\"<unk>\"])\n","  return indices\n","\n","# Defining a function that will take a list of indices and return a list of their corresponding words/tokens\n","def index_to_token(indices):\n","  sentence = []\n","  for ind in indices:\n","    sentence.append(index[ind])\n","  return sentence\n","\n","new_sentence = [\"cool\", \"guy\"]\n","token_to_index(new_sentence, word_index_pair)"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[8, 10]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFxA3HWZSKEl","executionInfo":{"status":"ok","timestamp":1627496167191,"user_tz":-300,"elapsed":42,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"0b29d830-2f22-403e-e953-86154216a187"},"source":["# Converting entire corpus into indices\n","corpus_indices = [token_to_index(sentence, word_index_pair) for sentence in preprocessed_corpus]\n","corpus_indices"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[13, 12, 2, 6],\n"," [14, 12, 2, 9],\n"," [16, 7, 23, 26],\n"," [15, 22, 25],\n"," [4, 17, 18],\n"," [19, 3, 21],\n"," [11, 24, 2, 5],\n"," [20, 12, 2, 8, 10]]"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0X0BNoftUOHy","executionInfo":{"status":"ok","timestamp":1627496167191,"user_tz":-300,"elapsed":39,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"ac64573b-a998-4d1a-fefa-378b6897bfe4"},"source":["reverted_corpus = [index_to_token(indices) for indices in corpus_indices]\n","reverted_corpus"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['jack', 'is', 'a', 'boy'],\n"," ['jill', 'is', 'a', 'girl'],\n"," ['josephine', 'broke', 'the', 'window'],\n"," ['joe', 'stood', 'watching'],\n"," ['andy', 'likes', 'martha'],\n"," ['rachel', 'and', 'ross'],\n"," ['henry', 'was', 'a', 'beast'],\n"," ['robert', 'is', 'a', 'cool', 'guy']]"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xTWDilUUSiql","executionInfo":{"status":"ok","timestamp":1627496167191,"user_tz":-300,"elapsed":36,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"9f4e76ae-4eeb-42a8-fc68-2fbc20fd2a2d"},"source":["# Once we have the indices of every word in the corpus, the next step is to create the word embeddings for each word in the corpus\n","# which will be stored in the form of a embedding table having shape N x E where N is the number of unique words in the corpus and E is the number of features per embedding\n","# To access the word embedding of a word, just index the embedding table using the index provided by the token_to_index() function\n","# An embedding matrix can be initialized randomly using the nn.Embedding() method which takes the arguments N and E\n","vocab_embeds = nn.Embedding(len(vocabulary), 4)\n","list(vocab_embeds.parameters())"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[-4.9031e-01,  8.0741e-02,  2.0764e+00,  8.3294e-01],\n","         [-1.3380e+00,  2.9521e-01, -1.7673e-01,  2.7782e-01],\n","         [ 9.1266e-01,  1.6677e-01, -8.0263e-01,  7.0830e-01],\n","         [-1.7560e+00, -1.0903e+00, -7.2437e-01,  7.3535e-01],\n","         [-1.8526e+00,  1.4280e+00, -2.7353e-01, -1.2659e+00],\n","         [-1.1594e-01, -1.5239e+00, -1.2135e+00,  8.4046e-01],\n","         [ 2.5654e-01,  4.2652e-01, -5.2919e-01,  1.0842e+00],\n","         [ 2.2344e-01,  1.5700e+00, -1.6874e-01, -6.1901e-01],\n","         [-1.2203e-01, -2.0465e-01,  1.2390e+00, -1.2371e+00],\n","         [ 9.7892e-02, -2.0413e+00, -2.6087e-01, -1.5696e+00],\n","         [-4.5248e-01,  1.1316e+00, -9.9751e-03,  1.5222e+00],\n","         [ 1.3787e+00, -3.6119e-01,  9.3978e-01,  5.2157e-01],\n","         [-1.9032e+00,  1.4626e+00, -4.8857e-01,  1.8613e+00],\n","         [ 1.4181e+00,  8.1234e-01, -1.8199e+00, -1.6225e-03],\n","         [ 5.6616e-01, -1.0337e+00,  1.5482e-01, -3.2893e+00],\n","         [-4.2602e-01,  4.9563e-01,  2.0227e+00,  1.1446e+00],\n","         [ 1.7855e+00, -1.7439e+00,  1.2680e+00, -1.4248e+00],\n","         [ 1.2417e+00,  9.1183e-01,  1.3519e+00,  4.0977e-01],\n","         [ 1.6384e+00,  4.9099e-01, -1.0973e+00, -1.8088e+00],\n","         [-8.1853e-01, -6.0728e-02,  3.3068e-01,  1.6347e-01],\n","         [ 7.3787e-01, -1.5842e+00,  1.8113e+00,  6.2000e-01],\n","         [-3.1507e-01,  1.1513e+00, -1.3450e-01, -2.3242e+00],\n","         [-8.5340e-01,  7.0993e-01,  2.4207e+00,  7.5106e-01],\n","         [ 1.4889e+00,  6.5148e-01,  5.7528e-01,  1.9953e+00],\n","         [ 3.2573e+00,  1.3610e-01,  1.3562e+00,  8.0833e-01],\n","         [-8.6584e-01, -1.2920e+00, -1.1977e+00,  1.4975e-03],\n","         [-3.3026e-01, -7.3264e-01, -1.1886e+00, -8.3015e-01]],\n","        requires_grad=True)]"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqeY7jhWVBpa","executionInfo":{"status":"ok","timestamp":1627496167191,"user_tz":-300,"elapsed":33,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"955c52ca-f1d6-4384-96d1-43418d4333f4"},"source":["# Mapping each word in the corpus to its respective embedding\n","sentence_embeds = []\n","for sentence in corpus_indices:\n","  sentence_embeds.append(vocab_embeds(torch.tensor(sentence, dtype=torch.long)))\n","sentence_embeds"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[ 1.4181e+00,  8.1234e-01, -1.8199e+00, -1.6225e-03],\n","         [-1.9032e+00,  1.4626e+00, -4.8857e-01,  1.8613e+00],\n","         [ 9.1266e-01,  1.6677e-01, -8.0263e-01,  7.0830e-01],\n","         [ 2.5654e-01,  4.2652e-01, -5.2919e-01,  1.0842e+00]],\n","        grad_fn=<EmbeddingBackward>),\n"," tensor([[ 0.5662, -1.0337,  0.1548, -3.2893],\n","         [-1.9032,  1.4626, -0.4886,  1.8613],\n","         [ 0.9127,  0.1668, -0.8026,  0.7083],\n","         [ 0.0979, -2.0413, -0.2609, -1.5696]], grad_fn=<EmbeddingBackward>),\n"," tensor([[ 1.7855, -1.7439,  1.2680, -1.4248],\n","         [ 0.2234,  1.5700, -0.1687, -0.6190],\n","         [ 1.4889,  0.6515,  0.5753,  1.9953],\n","         [-0.3303, -0.7326, -1.1886, -0.8301]], grad_fn=<EmbeddingBackward>),\n"," tensor([[-4.2602e-01,  4.9563e-01,  2.0227e+00,  1.1446e+00],\n","         [-8.5340e-01,  7.0993e-01,  2.4207e+00,  7.5106e-01],\n","         [-8.6584e-01, -1.2920e+00, -1.1977e+00,  1.4975e-03]],\n","        grad_fn=<EmbeddingBackward>),\n"," tensor([[-1.8526,  1.4280, -0.2735, -1.2659],\n","         [ 1.2417,  0.9118,  1.3519,  0.4098],\n","         [ 1.6384,  0.4910, -1.0973, -1.8088]], grad_fn=<EmbeddingBackward>),\n"," tensor([[-0.8185, -0.0607,  0.3307,  0.1635],\n","         [-1.7560, -1.0903, -0.7244,  0.7354],\n","         [-0.3151,  1.1513, -0.1345, -2.3242]], grad_fn=<EmbeddingBackward>),\n"," tensor([[ 1.3787, -0.3612,  0.9398,  0.5216],\n","         [ 3.2573,  0.1361,  1.3562,  0.8083],\n","         [ 0.9127,  0.1668, -0.8026,  0.7083],\n","         [-0.1159, -1.5239, -1.2135,  0.8405]], grad_fn=<EmbeddingBackward>),\n"," tensor([[ 0.7379, -1.5842,  1.8113,  0.6200],\n","         [-1.9032,  1.4626, -0.4886,  1.8613],\n","         [ 0.9127,  0.1668, -0.8026,  0.7083],\n","         [-0.1220, -0.2047,  1.2390, -1.2371],\n","         [-0.4525,  1.1316, -0.0100,  1.5222]], grad_fn=<EmbeddingBackward>)]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"VXsE-GbXbj14","executionInfo":{"status":"ok","timestamp":1627496167192,"user_tz":-300,"elapsed":32,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# Next, we use the DataLoader function from torch.utils.data to breakup the data into batches that can be used to learn the network\n","# The DataLoader function takes the following arguments:\n","# 1 - The data - this represents the entire dataset, features and labels combined\n","# 2 - Batch Size - this represents the number of examples to learn per batch. Over an epoch, all the batches will be iterated\n","# 3 - Shuffle - this represents whether the order of batches is to be shuffled before each sampling iteration\n","# 4 - collate_fn - this represents the collate function that will be passed to the DataLoader.\n","# A collate function is a function that is used either to print the stats of a batch or to perform further processing on it before it is passed as input to the network\n","# The collate function is applied to the current batch so it always takes it as an argument. Additional arguments may be added as needed\n","\n","# Importing DataLoader and partial function\n","# Partial function is used to hard-code some of the arguments to a function\n","from torch.utils.data import DataLoader\n","from functools import partial\n","\n","# Defining custom collate function\n","def collate_func(batch, window_size, word_to_index):\n","  x, y = zip(*batch)\n","\n","  # Padding all sentences first so that each word can be windowed succesfully\n","  # And then converting each tokenized sentence to an indexed sentence\n","  x_padded = [pad_sentence(x_i, window_size=window_size) for x_i in x]\n","  x_indexed = [token_to_index(x_i, word_to_index=word_to_index) for x_i in x_padded]\n","  x_tensor = [torch.LongTensor(x_i) for x_i in x_indexed]\n","  pad_index = word_to_index[\"<pad>\"]\n","  # Padding again to ensure all sentences are of same length\n","  x_pad_indexed = nn.utils.rnn.pad_sequence(x_tensor, batch_first = True, padding_value = pad_index)\n","\n","  # Remembering the lengths of each sentence without padding to avoid inaccurate loss\n","  lengths = [len(y_i) for y_i in y]\n","  lengths_tensor = torch.LongTensor(lengths)\n","  y_tensor = [torch.LongTensor(y_i) for y_i in y]\n","  # Padding all y's to be of same length using nn.utils.rnn.pad_sequence()\n","  # y not padded to fit window size because it won't be windowed\n","  y_padded = nn.utils.rnn.pad_sequence(y_tensor, batch_first=True, padding_value = 0)\n","\n","  return x_pad_indexed, y_padded, lengths_tensor\n","\n","\n","  \n"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5PmVVg_mVTr","executionInfo":{"status":"ok","timestamp":1627496167192,"user_tz":-300,"elapsed":31,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"035ef8b2-8efe-4b9b-99b3-7557272e926c"},"source":["data = list(zip(preprocessed_corpus, corpus_labels))\n","batch_size = 2\n","window_size = 2\n","shuffle=True\n","f = partial(collate_func, window_size = window_size, word_to_index = word_index_pair)\n","\n","loader = DataLoader(data, batch_size = batch_size, shuffle=shuffle, collate_fn = f)\n","\n","for x_batch, y_batch, l_batch in loader:\n","  print(\"Input X batch: \")\n","  print(x_batch)\n","  print(\"Output Y labels: \")\n","  print(y_batch)\n","  print(\"Lengths: \")\n","  print(l_batch)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["Input X batch: \n","tensor([[ 0,  0, 16,  7, 23, 26,  0,  0],\n","        [ 0,  0, 11, 24,  2,  5,  0,  0]])\n","Output Y labels: \n","tensor([[1, 0, 0, 0],\n","        [1, 0, 0, 0]])\n","Lengths: \n","tensor([4, 4])\n","Input X batch: \n","tensor([[ 0,  0, 20, 12,  2,  8, 10,  0,  0],\n","        [ 0,  0, 15, 22, 25,  0,  0,  0,  0]])\n","Output Y labels: \n","tensor([[1, 0, 0, 0, 0],\n","        [1, 0, 0, 0, 0]])\n","Lengths: \n","tensor([5, 3])\n","Input X batch: \n","tensor([[ 0,  0, 19,  3, 21,  0,  0,  0],\n","        [ 0,  0, 14, 12,  2,  9,  0,  0]])\n","Output Y labels: \n","tensor([[1, 0, 1, 0],\n","        [1, 0, 0, 0]])\n","Lengths: \n","tensor([3, 4])\n","Input X batch: \n","tensor([[ 0,  0,  4, 17, 18,  0,  0,  0],\n","        [ 0,  0, 13, 12,  2,  6,  0,  0]])\n","Output Y labels: \n","tensor([[1, 0, 0, 0],\n","        [1, 0, 0, 0]])\n","Lengths: \n","tensor([3, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coe3adDGwamm","executionInfo":{"status":"ok","timestamp":1627496167192,"user_tz":-300,"elapsed":29,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"21dc46fc-7d98-4316-cf34-428517367df1"},"source":["# Now that we are succesfully able to create batches of data, we need to be able to split them into windows.\n","# For a sentence with the n non-padded tokens, the number of windows created will be n\n","# Each window will be of size 2N + 1, where N is the window size (2N because there'll be N tokens either side of the center token, and the center token itself counts as 1 so +1)\n","# PyTorch provides a function unfold() that performs this task and takes the following arguments:\n","# 1 - Dimension - this is the dimension in which the windows are to be created\n","# 2 - Window size - this is the window size and is 2N + 1\n","# 3 - Step - This is the step that is to be taken after each window is sliced\n","# The unfold() method can be called on any tensor using the dot operator\n","x_unfold = x_batch.unfold(1, 2*2 + 1, 1)\n","x_unfold"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  0,  4, 17, 18],\n","         [ 0,  4, 17, 18,  0],\n","         [ 4, 17, 18,  0,  0],\n","         [17, 18,  0,  0,  0]],\n","\n","        [[ 0,  0, 13, 12,  2],\n","         [ 0, 13, 12,  2,  6],\n","         [13, 12,  2,  6,  0],\n","         [12,  2,  6,  0,  0]]])"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"GGXAs-iGyypr","executionInfo":{"status":"ok","timestamp":1627496167192,"user_tz":-300,"elapsed":26,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# We will use the unfold() function in the development of our model\n","# The model architecture is going to be follows:\n","# 1 - The first layer will be an embedding layer in which the following operations will be carried out:\n","#     a) Each input batch will be windowed using the unfold() function, therefore the input matrix will become of shape B x L~ x W, where B is the batch size, L~ represents the number of windows (L is the length of the padded sentence), and W represents the window size (2N + 1)\n","#     b) Next, each token of each window will be looked up in the embedding table and replaced by their respective embedding, therefore the input matrix will become of shape B x L~ x W x D, where B, L~, and W are the same as before and D represents the number of dimensions per embedding\n","# 2 - The second layer will be a linear layer called the hidden layer. This layer will perform a linear operation on the embeddings matrix and will then apply the tanh() non_linearity.\n","# The purpose of this layer will be to represent each window (W x D) collectively as a whole in a single dimension which will be the output dimension H_out of this layer and will be denoted with H. So, the output matrix of the hidden layer will be of shape B x L~ x H\n","# 3 - The third layer will be another linear layer called the output layer. This layer will perform a linear operation on the output of the hidden layer and will shrink the information carried in the Hth dimension of the hidden layer output to a single scalar (each window will be represented using a single scalar). Therefore, the output matrix of this layer will be of shape B x L~ x 1.\n","# 4 - The 4th and final layer will be a sigmoid non-linearity that will be added to convert the learnt window information into a probability. These probabilities will then be output by the model with a higher probability meaning that a name in that location is more likely than any other word\n","class WordWindowClassifier(nn.Module):\n","  def __init__(self, hyperparameters, vocab_size, padding_ind=0):\n","    super(WordWindowClassifier, self).__init__()\n","    self.window_size = hyperparameters[\"window_size\"]\n","    self.embedding_dimensions = hyperparameters[\"embed_dim\"]\n","    self.hidden_dimensions = hyperparameters[\"hidden_dim\"]\n","\n","    # Defining window width = 2N + 1\n","    self.window_width = 2 * window_size + 1\n","    # Defining a randomly initialized embedding matrix having dimensions n x D, where n is the number of words in the vocabulary and D is the number of embedding dimensions per word\n","    self.embeddings = nn.Embedding(vocab_size, self.embedding_dimensions, padding_idx = padding_ind)\n","    # Defining the hidden layer as a linear layer with input features W x D and output features H followed by a tanh() non-linearity\n","    self.hidden_layer = nn.Sequential(\n","        nn.Linear(self.window_width * self.embedding_dimensions, self.hidden_dimensions),\n","        nn.Tanh())\n","    # Defining the output layer as a linear layer with input features H and output features 1\n","    self.output_layer = nn.Linear(self.hidden_dimensions, 1)\n","    # Defining the sigmoid() non-linearity\n","    self.probability = nn.Sigmoid()\n","  def forward(self, inputs):\n","    # Getting the dimensions of the input - Batch Size x Length of each batch\n","    B, L = inputs.size()\n","    # Converting each token of each batch into its respective window, input matrix now has dimensions B x L~ x W\n","    tokenized_inputs_windowed = inputs.unfold(1, self.window_width, 1)\n","    # Extracting the number of windows of every batch example (extracting L~)\n","    _, unpadded_length, _ = tokenized_inputs_windowed.size()\n","    # Converting each window token into its respective embedding, input matrix now has dimensions B x L~ x W x D\n","    tokenized_windowed_embeddings = self.embeddings(tokenized_inputs_windowed)\n","    # Reshaping input matrix so that the last 2 dimensions can be merged into a single dimension, input matrix now has dimensions B x L~ x (W * D)\n","    tokenized_windowed_embeddings = tokenized_windowed_embeddings.view(B, unpadded_length, -1)\n","    # Passing the input into the hidden layer, output matrix of this layer has dimensions B x L~ x H, where H is the output features of the layer\n","    H_output = self.hidden_layer(tokenized_windowed_embeddings)\n","    # Passing the output of the hidden layer into the output layer, output matrix of this layer has dimensions B x L~ x 1\n","    F_output = self.output_layer(H_output)\n","    # Converting the output of the layer into probabilities\n","    output = self.probability(F_output)\n","    output = output.view(B, -1)\n","    return output"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1k_RLOx385Ry","executionInfo":{"status":"ok","timestamp":1627496198098,"user_tz":-300,"elapsed":30932,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"9e0f0fad-6902-4fc5-c0d8-c39f770d61b0"},"source":["# Now that the modelling has finished and the final architecture implemented, the next step is to train the model\n","# To train the model, we will first need to use the DataLoader to generate batches of our data\n","\n","# Initializing arguments to DataLoader\n","data = list(zip(preprocessed_corpus, corpus_labels))\n","shuffle = True\n","batch_size = 2\n","window_size = 2\n","f = partial(collate_func, window_size = window_size, word_to_index = word_index_pair)\n","\n","# Initializing DataLoader object\n","loader = DataLoader(data, batch_size=batch_size, shuffle = shuffle, collate_fn=f)\n","\n","#Initializing a dictionary of the hyperparameters of the model\n","hyperparameters = {\n","    \"window_size\": 2,\n","    \"embed_dim\": 25,\n","    \"hidden_dim\": 25,\n","}\n","\n","# Initializing the model\n","model = WordWindowClassifier(hyperparameters, len(vocabulary), padding_ind = 0)\n","# Initializing the optimizer\n","lr = 0.01\n","SGD = optim.SGD(model.parameters(), lr = lr)\n","# Defining a custom loss function that will use nn.BCELoss(), cannot use BCELoss() directly because outputs are padded\n","def loss_func(preds, labels, batch_lengths):\n","  loss = nn.BCELoss()\n","  loss_val = loss(preds, labels.float())\n","  loss_val = loss_val / batch_lengths.sum().float()\n","  return loss_val\n","\n","# Defining a training function to train the model for n number of epochs\n","def train(loader, optimizer, model, n_epochs=10000):\n","  for i in range(n_epochs):\n","    for x_batch, y_batch, lengths_batch in loader:\n","      optimizer.zero_grad()\n","      preds = model(x_batch)\n","      loss = loss_func(preds, y_batch, lengths_batch)\n","      loss.backward()\n","      optimizer.step()\n","    if i % 1000 == 0:\n","      print(loss.item())\n","\n","train(loader, SGD, model)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["0.10533452779054642\n","0.009202997200191021\n","0.0012022750452160835\n","0.001559594296850264\n","0.0012242839438840747\n","0.000683983729686588\n","0.0009183045476675034\n","0.0003013168752659112\n","0.000336874567437917\n","0.0005727809038944542\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0VQpTHVc8yr","executionInfo":{"status":"ok","timestamp":1627496198099,"user_tz":-300,"elapsed":9,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"fc050542-7436-4ba5-a79d-d5095f222eb3"},"source":["# Defining some test data\n","test_corpus = [\"Jacky is a little kid\", \"John is a jock\"]\n","test_corpus = [sentence.lower().split() for sentence in test_corpus]\n","test_labels = [[1, 0, 0, 0, 0], [1, 0, 0, 0]]\n","batch_size = 1\n","shuffle = True\n","window_size = 2\n","test_data = list(zip(test_corpus, test_labels))\n","f = partial(collate_func, window_size=window_size, word_to_index = word_index_pair)\n","loader = DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, collate_fn = f)\n","for x_batch, y_batch, _ in loader:\n","  preds = model(x_batch)\n","  print(y_batch)\n","  print(preds)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["tensor([[1, 0, 0, 0, 0]])\n","tensor([[0.9925, 0.0014, 0.0606, 0.0023, 0.0039]], grad_fn=<ViewBackward>)\n","tensor([[1, 0, 0, 0]])\n","tensor([[9.9250e-01, 1.3878e-03, 1.3681e-01, 7.0494e-04]],\n","       grad_fn=<ViewBackward>)\n"],"name":"stdout"}]}]}