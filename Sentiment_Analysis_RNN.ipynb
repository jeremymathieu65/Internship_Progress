{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kanrED4tv-4A",
        "outputId": "7cf6eccf-c942-4559-cbbf-33368cd3724b"
      },
      "source": [
        "# Importing Libraries and Functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Reading in the dataset from Google Drive (Train-Test split is implemented in an 80-20 ratio)\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/FinancialData.csv', names = [\"Sentiment\", \"Review\"], encoding='utf-8')\n",
        "training_data = dataset.sample(frac = 0.8, random_state = 42)\n",
        "test_data = dataset.drop(training_data.index)\n",
        "corpus = dataset[\"Review\"]\n",
        "# Splitting corpus and labels from training and test sets\n",
        "training_corpus = list(training_data[\"Review\"])\n",
        "training_labels = list(training_data[\"Sentiment\"])\n",
        "test_corpus = list(test_data[\"Review\"])\n",
        "test_labels = list(test_data[\"Sentiment\"])\n",
        "\n",
        "# Defining a function to preprocess the corpus - convert all letters to lowercase and splitting on the occurence of spaces, removing stop words and numbers\n",
        "def preprocess(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence  = nltk.word_tokenize(sentence.lower())\n",
        "    punc_free_sent = [token for token in tokenized_sentence if token.isalpha()]\n",
        "    keepers = [\"up\", \"down\", \"off\", \"on\", \"above\", \"below\", \"too\", \"very\", \"between\", \"against\", \"between\", \"same\", \"not\", \"no\", \"only\", \"too\", \"very\"]\n",
        "    stop_words = [word for word in stopwords.words(\"english\") if word not in keepers]\n",
        "    stop_free_sent = [token for token in punc_free_sent if token not in stopwords.words('english')]\n",
        "    if len(stop_free_sent) != 0:\n",
        "      tokenized_corpus.append(stop_free_sent)\n",
        "  return tokenized_corpus\n",
        "\n",
        "corpus = preprocess(corpus)\n",
        "training_corpus = preprocess(training_corpus)\n",
        "test_corpus = preprocess(test_corpus)\n",
        "\n",
        "# Defining a tensor of lengths of sequence pre-padding so that loss calculation is accurate\n",
        "training_lengths = []\n",
        "for sentence in training_corpus:\n",
        "  training_lengths.append(len(sentence))\n",
        "training_lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "test_lengths = []\n",
        "for sentence in test_corpus:\n",
        "  test_lengths.append(len(sentence))\n",
        "test_lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "\n",
        "# Mapping each sentiment to a number -1 = Negative, 0 = Neutral, 1 = Positive\n",
        "def map_sentiment(labels):\n",
        "  mapped_labels = []\n",
        "  for label in labels:\n",
        "    if label == \"negative\": mapped_labels.append(-1)\n",
        "    elif label == \"neutral\": mapped_labels.append(0)\n",
        "    else: mapped_labels.append(1)\n",
        "  return mapped_labels\n",
        "\n",
        "training_labels = map_sentiment(training_labels)\n",
        "test_labels = map_sentiment(test_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkIP8HY6EhIZ",
        "outputId": "9f60c448-1f99-402a-f910-89dd21653fab"
      },
      "source": [
        "# Training a word2vec model for the training corpus\n",
        "word_embeds = Word2Vec(corpus, size = 75, min_count = 1)\n",
        "\n",
        "# Mapping each token of each sentence (sequence) to its respective embedding\n",
        "def map_sequence(corpus, embeds):\n",
        "  corpus_embeds = []\n",
        "  for sentence in corpus:\n",
        "    sentence_embeds = []\n",
        "    for token in sentence:\n",
        "      sentence_embeds.append(torch.from_numpy(embeds[token]))\n",
        "    sentence_embeds = torch.stack(sentence_embeds, dim=0)\n",
        "    corpus_embeds.append(sentence_embeds)\n",
        "  corpus_embeds = pad_sequence(corpus_embeds, batch_first = True)\n",
        "  return corpus_embeds\n",
        "\n",
        "corpus_embeds = map_sequence(training_corpus, word_embeds)\n",
        "test_embeds = map_sequence(test_corpus, word_embeds)\n",
        "# Creating a dictionary of hyperparameters for the model\n",
        "hyperparameters = {\n",
        "    \"input_size\": 75,\n",
        "    \"hidden_size\": 256,\n",
        "    \"sequence_length\": 35,\n",
        "    \"num_layers\": 3,\n",
        "    \"batch_size\": 256,\n",
        "    \"num_classes\": 3,\n",
        "    \"num_epochs\": 1000,\n",
        "    \"learning_rate\": 0.001,\n",
        "}\n",
        "\n",
        "# Defining the architecture of the model\n",
        "class SentimentRNN(nn.Module):\n",
        "  def __init__(self, hyperparameters):\n",
        "    super(SentimentRNN, self).__init__()\n",
        "    self.input_size = hyperparameters[\"input_size\"]\n",
        "    self.hidden_size = hyperparameters[\"hidden_size\"]\n",
        "    self.num_layers = hyperparameters[\"num_layers\"]\n",
        "    self.num_classes = hyperparameters[\"num_classes\"]\n",
        "    self.sequence_length = hyperparameters[\"sequence_length\"]\n",
        "    # Creating hidden layer for each token, output matrix will be B x L x H, 256 x 35 x 256\n",
        "    self.recurrent_layer = nn.RNN(self.input_size, self.hidden_size, num_layers = self.num_layers, bidirectional=True, batch_first=True)\n",
        "    # Adding a final linear layer followed by the Sigmoid() non-linearity to make a probability distribution, outputs a matrix of dimensions B x 1 - 256 x 1\n",
        "    self.linear_layer = nn.Sequential(\n",
        "        nn.Linear(self.hidden_size * 2 * self.sequence_length, 1),\n",
        "        nn.Sigmoid())\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    hidden_output, _ = self.recurrent_layer(inputs)\n",
        "    hidden_output = hidden_output.reshape(hidden_output.shape[0], -1)\n",
        "    linear_output = self.linear_layer(hidden_output)\n",
        "    linear_output = linear_output.squeeze(1)\n",
        "    return linear_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3876, 35, 75])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPh3V8Tyo33e",
        "outputId": "9e2f97bd-ca07-4d66-fee5-99824ec38fb7"
      },
      "source": [
        "# Parsing the data into batches using DataLoader\n",
        "data = list(zip(corpus_embeds, training_labels, lengths))\n",
        "loader = DataLoader(data, batch_size = hyperparameters[\"batch_size\"], shuffle = True)\n",
        "\n",
        "# Defining a custom loss function to accont for variable length\n",
        "def loss_func(y, preds, length):\n",
        "  loss = nn.BCELoss()\n",
        "  loss_val = loss(preds, y.float())\n",
        "  loss_val = loss_val / length.sum().float()\n",
        "  return loss_val\n",
        "\n",
        "# Initializing the model\n",
        "model = SentimentRNN(hyperparameters)\n",
        "# Initializing an SGD optimizer\n",
        "SGD = optim.SGD(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "def train(model, optimizer, loader, nepoch=100):\n",
        "  for i in range(nepoch):\n",
        "    for x_batch, y_batch, length_batch in loader:\n",
        "      SGD.zero_grad()\n",
        "      preds = model(x_batch)\n",
        "      loss = loss_func(y_batch, preds, length_batch)\n",
        "      loss.backward()\n",
        "      SGD.step()\n",
        "    if i % 10 == 0:\n",
        "      print(loss)\n",
        "\n",
        "train(model, SGD, loader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0015, grad_fn=<DivBackward0>)\n",
            "tensor(0.0016, grad_fn=<DivBackward0>)\n",
            "tensor(0.0016, grad_fn=<DivBackward0>)\n",
            "tensor(0.0019, grad_fn=<DivBackward0>)\n",
            "tensor(0.0016, grad_fn=<DivBackward0>)\n",
            "tensor(0.0016, grad_fn=<DivBackward0>)\n",
            "tensor(0.0014, grad_fn=<DivBackward0>)\n",
            "tensor(0.0015, grad_fn=<DivBackward0>)\n",
            "tensor(0.0017, grad_fn=<DivBackward0>)\n",
            "tensor(0.0017, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}