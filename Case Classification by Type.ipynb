{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Case Classification by Type.ipynb","provenance":[],"mount_file_id":"1tOTAu4k7XE0n_1WHTzLi-XycC3ahYfVU","authorship_tag":"ABX9TyOYZ0Wf9C80/+0+a4Fx3BvR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tk27EHJCZJ9r","executionInfo":{"status":"ok","timestamp":1628192161668,"user_tz":-300,"elapsed":17983,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"8a6854b1-6185-4792-b2c9-ef543f7c575a"},"source":["# Importing required libraries\n","!pip install transformers\n","import transformers\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, TrainingArguments, Trainer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from functools import partial\n","import pandas as pd\n","import os\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words(\"english\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RJzop8JNeJ9n","executionInfo":{"status":"ok","timestamp":1628192161669,"user_tz":-300,"elapsed":13,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# # Importing data\n","# # Data is a collection of case reports from cases in the USA with each case classified into its type.\n","# # Data is extracted and stored into a new directory called Cases/\n","# !unzip drive/MyDrive/'Colab Notebooks/US Court Cases.zip'\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDKTrAm9lrch","executionInfo":{"status":"ok","timestamp":1628192161669,"user_tz":-300,"elapsed":12,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# # Extracting label information for each case type (all 78 case types):\n","# labels = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Map.txt\", sep=\"->\", names = [\"Label Number\", \"Case Type\"])\n","# labels.head(10)\n","\n","# # Creating a blank data frame to store upcoming data\n","# dataset = pd.DataFrame(columns = [\"label\", \"text\"])\n","\n","# # Extracting Case Information\n","# for i in range(1, 79):\n","#   if i != 48 and i != 46:\n","#     file_names = os.listdir(\"/content/Cases/\" + str(i))\n","#     for file_name in file_names:\n","#       with open(\"/content/Cases/\" + str(i) + \"/\" + file_name, 'r') as file:\n","#         case_text = file.read().replace('\\n', ' ')\n","#       case_info = {\n","#           \"label\": i,\n","#           \"text\": case_text,\n","#       }\n","#       dataset = dataset.append(case_info, ignore_index = True)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3WFhI__Z7T3","executionInfo":{"status":"ok","timestamp":1628192161669,"user_tz":-300,"elapsed":11,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# Exporting the dataset as a csv to avoid unzipping and extracting every time\n","# dataset.to_csv(\"/content/drive/MyDrive/Colab Notebooks/US_Court_Cases.csv\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1EfcsklybHc","executionInfo":{"status":"ok","timestamp":1628192185937,"user_tz":-300,"elapsed":24279,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["# Randomly sampling training and test sets in 80-20 ratio\n","dataset = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/US_Court_Cases.csv\")\n","training_data = dataset.sample(frac = 0.8, random_state = 42)\n","test_data = dataset.drop(training_data.index)\n","train_data = training_data.sample(frac = 0.8, random_state = 42)\n","validation_data = training_data.drop(train_data.index)\n","train_data = train_data[0:500]\n","test_data = test_data[0:150]\n","validation_data = validation_data[0:150]\n","train_labels = train_data[\"label\"].to_list()\n","train_text = train_data[\"text\"].to_list()\n","test_labels = test_data[\"label\"].to_list()\n","test_text = test_data[\"text\"].to_list()\n","validation_labels = validation_data[\"label\"].to_list()\n","validation_text = validation_data[\"text\"].to_list()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VypgAQtBaol_","executionInfo":{"status":"ok","timestamp":1628192207210,"user_tz":-300,"elapsed":21284,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["def pre_process(corpus):\n","  corpus_tokenized = []\n","  for document in corpus:\n","    document_tokenized = []\n","    if type(document) == str:\n","      tokens = nltk.word_tokenize(document)\n","      for token in tokens:\n","        if token not in stop_words and token.isalpha():\n","          document_tokenized.append(token)\n","      corpus_tokenized.append(document_tokenized)\n","  return corpus_tokenized\n","\n","training_corpus = pre_process(train_text)\n","validation_corpus = pre_process(validation_text)\n","test_corpus = pre_process(test_text)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKNMk49pevXA","executionInfo":{"status":"ok","timestamp":1628192228955,"user_tz":-300,"elapsed":21754,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}},"outputId":"b284e5c5-345d-421f-b1af-3ab2f686bf5b"},"source":["tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n","\n","train_encodings = tokenizer(training_corpus, is_split_into_words = True, padding = True, truncation = True)\n","validation_encodings = tokenizer(validation_corpus, is_split_into_words = True, padding = True, truncation = True)\n","test_encodings = tokenizer(test_corpus, is_split_into_words = True, padding = True, truncation = True)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"S2lzjihL41cK","executionInfo":{"status":"ok","timestamp":1628192228957,"user_tz":-300,"elapsed":11,"user":{"displayName":"Jeremy Mathieu","photoUrl":"","userId":"03857391246187283771"}}},"source":["class CourtCasesDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings, labels):\n","    self.encodings = encodings\n","    self.labels = labels\n","\n","  def __getitem__(self, idx):\n","    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    item[\"labels\"] = torch.tensor(self.labels[idx])\n","    return item\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","train_dataset = CourtCasesDataset(train_encodings, train_labels)\n","validation_dataset = CourtCasesDataset(validation_encodings, validation_labels)\n","test_dataset = CourtCasesDataset(test_encodings, test_labels)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIV3LPdX_8nB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5680c11e-2a26-4a28-8d69-4f3febc04ef7"},"source":["# del dataset, training_data, train_data, test_data, validation_data, train_text, test_text, validation_text, train_labels, test_labels, validation_labels,\n","# del training_corpus, test_corpus, validation_corpus, train_encodings, test_encodings, validation_encodings\n","\n","training_args = TrainingArguments(\n","    output_dir = \"/content/drive/MyDrive/Colab Notebooks\",\n","    num_train_epochs = 3,\n","    per_device_train_batch_size = 64,\n","    per_device_eval_batch_size = 256,\n","    warmup_steps = 150,\n","    weight_decay = 0.1,\n","    logging_dir = \"/content/drive/MyDrive/Colab Notebooks/logs\",\n","    logging_steps = 10,\n",")\n","\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = train_dataset, \n","    eval_dataset = validation_dataset\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 24\n"],"name":"stderr"}]}]}