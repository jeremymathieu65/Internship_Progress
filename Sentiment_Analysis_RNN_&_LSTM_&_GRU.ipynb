{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis RNN & LSTM & GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kanrED4tv-4A",
        "outputId": "7a9c0dad-b56e-44c5-a2db-baaf03527b9f"
      },
      "source": [
        "# Importing Libraries and Functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Reading in the dataset from Google Drive (Train-Test split is implemented in an 80-20 ratio)\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/FinancialData.csv', names = [\"Sentiment\", \"Review\"], encoding='utf-8')\n",
        "training_data = dataset.sample(frac = 0.8, random_state = 42)\n",
        "training_data = training_data.drop(index = 4845)\n",
        "test_data = dataset.drop(training_data.index)\n",
        "test_data = test_data.drop(index = 4845)\n",
        "# Splitting corpus and labels from training and test sets\n",
        "training_corpus = list(training_data[\"Review\"])\n",
        "test_corpus = list(test_data[\"Review\"])\n",
        "training_labels = list(training_data[\"Sentiment\"])\n",
        "test_labels = list(test_data[\"Sentiment\"])\n",
        "\n",
        "# Defining a tensor of lengths of sequence pre-padding so that loss calculation is accurate\n",
        "training_lengths = []\n",
        "for sentence in training_corpus:\n",
        "  training_lengths.append(len(sentence))\n",
        "training_lengths = torch.tensor(training_lengths, dtype=torch.float)\n",
        "test_lengths = []\n",
        "for sentence in test_corpus:\n",
        "  test_lengths.append(len(sentence))\n",
        "test_lengths = torch.tensor(training_lengths, dtype=torch.float)\n",
        "\n",
        "# Mapping each sentiment to a number -1 = Negative, 0 = Neutral, 1 = Positive\n",
        "def map_sentiment(labels):\n",
        "  mapped_labels = []\n",
        "  for label in labels:\n",
        "    if label == \"negative\": mapped_labels.append(-1)\n",
        "    elif label == \"neutral\": mapped_labels.append(0)\n",
        "    else: mapped_labels.append(1)\n",
        "  return mapped_labels\n",
        "\n",
        "training_labels = map_sentiment(training_labels)\n",
        "test_labels = map_sentiment(test_labels)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkIP8HY6EhIZ",
        "outputId": "1b645c4e-0e63-4f54-f5b3-846de22e2b1b"
      },
      "source": [
        "corpus = dataset[\"Review\"]\n",
        "# Defining a function to preprocess the corpus - convert all letters to lowercase and splitting on the occurence of spaces, removing stop words and numbers\n",
        "def preprocess(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence  = nltk.word_tokenize(sentence.lower())\n",
        "    punc_free_sent = [token for token in tokenized_sentence if token.isalpha()]\n",
        "    keepers = [\"up\", \"down\", \"off\", \"on\", \"above\", \"below\", \"too\", \"very\", \"between\", \"against\", \"between\", \"same\", \"not\", \"no\", \"only\", \"too\", \"very\"]\n",
        "    stop_words = [word for word in stopwords.words(\"english\") if word not in keepers]\n",
        "    stop_free_sent = [token for token in punc_free_sent if token not in stopwords.words('english')]\n",
        "    if len(stop_free_sent) != 0:\n",
        "      tokenized_corpus.append(stop_free_sent)\n",
        "  return tokenized_corpus\n",
        "\n",
        "corpus = preprocess(corpus)\n",
        "\n",
        "# Training a word2vec model for the training corpus\n",
        "word_embeds = Word2Vec(corpus, size = 75, min_count = 1)\n",
        "\n",
        "# Mapping each token of each sentence (sequence) to its respective embedding\n",
        "def map_sequence(corpus, embeds):\n",
        "  corpus_embeds = []\n",
        "  for sentence in corpus:\n",
        "    sentence_embeds = []\n",
        "    for token in sentence:\n",
        "      sentence_embeds.append(torch.from_numpy(embeds[token]))\n",
        "    sentence_embeds = torch.stack(sentence_embeds, dim=0)\n",
        "    corpus_embeds.append(sentence_embeds)\n",
        "  corpus_embeds = pad_sequence(corpus_embeds, batch_first = True)\n",
        "  return corpus_embeds\n",
        "\n",
        "corpus = map_sequence(corpus, word_embeds)\n",
        "training_corpus = []\n",
        "test_corpus = []\n",
        "index = training_data.index.to_list()\n",
        "for i in index:\n",
        "  # Filtering out of bounds indices\n",
        "  training_corpus.append(corpus[i])\n",
        "index = test_data.index.to_list()\n",
        "for i in index:\n",
        "  test_corpus.append(corpus[i])\n",
        "\n",
        "# training_embeds = map_sequence(training_corpus, word_embeds)\n",
        "# test_embeds = map_sequence(test_corpus, word_embeds)\n",
        "# Creating a dictionary of hyperparameters for the model\n",
        "hyperparameters = {\n",
        "    \"input_size\": 75,\n",
        "    \"hidden_size\": 256,\n",
        "    \"sequence_length\": 35,\n",
        "    \"num_layers\": 3,\n",
        "    \"batch_size\": 256,\n",
        "    \"num_classes\": 3,\n",
        "    \"num_epochs\": 1000,\n",
        "    \"learning_rate\": 0.001,\n",
        "}\n",
        "\n",
        "# Defining the architecture of the simple RNN model\n",
        "class SentimentRNN(nn.Module):\n",
        "  def __init__(self, hyperparameters):\n",
        "    super(SentimentRNN, self).__init__()\n",
        "    self.input_size = hyperparameters[\"input_size\"]\n",
        "    self.hidden_size = hyperparameters[\"hidden_size\"]\n",
        "    self.num_layers = hyperparameters[\"num_layers\"]\n",
        "    self.num_classes = hyperparameters[\"num_classes\"]\n",
        "    self.sequence_length = hyperparameters[\"sequence_length\"]\n",
        "    # Creating hidden layer for each token, output matrix will be B x L x H, 256 x 35 x 256\n",
        "    self.recurrent_layer = nn.RNN(self.input_size, self.hidden_size, num_layers = self.num_layers, bidirectional=True, batch_first=True)\n",
        "    # Adding a final linear layer followed by the Sigmoid() non-linearity to make a probability distribution, outputs a matrix of dimensions B x 1 - 256 x 1\n",
        "    self.linear_layer = nn.Sequential(\n",
        "        nn.Linear(self.hidden_size * 2 * self.sequence_length, 1),\n",
        "        nn.Sigmoid())\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    hidden_output, _ = self.recurrent_layer(inputs)\n",
        "    hidden_output = hidden_output.reshape(hidden_output.shape[0], -1)\n",
        "    linear_output = self.linear_layer(hidden_output)\n",
        "    linear_output = linear_output.squeeze(1)\n",
        "    return linear_output\n",
        "\n",
        "# Defining the architecture of the LSTM model\n",
        "class SentimentLSTM(nn.Module):\n",
        "  def __init__(self, hyperparameters):\n",
        "    super(SentimentLSTM, self).__init__()\n",
        "    self.input_size = hyperparameters[\"input_size\"]\n",
        "    self.hidden_size = hyperparameters[\"hidden_size\"]\n",
        "    self.num_layers = hyperparameters[\"num_layers\"]\n",
        "    self.num_classes = hyperparameters[\"num_classes\"]\n",
        "    self.sequence_length = hyperparameters[\"sequence_length\"]\n",
        "    # Creating hidden layer for each token, output matrix will be B x L x H, 256 x 35 x 256\n",
        "    self.recurrent_layer = nn.LSTM(self.input_size, self.hidden_size, num_layers = self.num_layers, bidirectional=True, batch_first=True)\n",
        "    # Adding a final linear layer followed by the Sigmoid() non-linearity to make a probability distribution, outputs a matrix of dimensions B x 1 - 256 x 1\n",
        "    self.linear_layer = nn.Sequential(\n",
        "        nn.Linear(self.hidden_size * 2 * self.sequence_length, 1),\n",
        "        nn.Sigmoid())\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    hidden_output, _ = self.recurrent_layer(inputs)\n",
        "    hidden_output = hidden_output.reshape(hidden_output.shape[0], -1)\n",
        "    linear_output = self.linear_layer(hidden_output)\n",
        "    linear_output = linear_output.squeeze(1)\n",
        "    return linear_output\n",
        "\n",
        "# Defining the architecture of the GRU model\n",
        "class SentimentGRU(nn.Module):\n",
        "  def __init__(self, hyperparameters):\n",
        "    super(SentimentGRU, self).__init__()\n",
        "    self.input_size = hyperparameters[\"input_size\"]\n",
        "    self.hidden_size = hyperparameters[\"hidden_size\"]\n",
        "    self.num_layers = hyperparameters[\"num_layers\"]\n",
        "    self.num_classes = hyperparameters[\"num_classes\"]\n",
        "    self.sequence_length = hyperparameters[\"sequence_length\"]\n",
        "    # Creating hidden layer for each token, output matrix will be B x L x H, 256 x 35 x 256\n",
        "    self.recurrent_layer = nn.GRU(self.input_size, self.hidden_size, num_layers = self.num_layers, bidirectional=True, batch_first=True)\n",
        "    # Adding a final linear layer followed by the Sigmoid() non-linearity to make a probability distribution, outputs a matrix of dimensions B x 1 - 256 x 1\n",
        "    self.linear_layer = nn.Sequential(\n",
        "        nn.Linear(self.hidden_size * 2 * self.sequence_length, 1),\n",
        "        nn.Sigmoid())\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    hidden_output, _ = self.recurrent_layer(inputs)\n",
        "    hidden_output = hidden_output.reshape(hidden_output.shape[0], -1)\n",
        "    linear_output = self.linear_layer(hidden_output)\n",
        "    linear_output = linear_output.squeeze(1)\n",
        "    return linear_output\n",
        "\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPh3V8Tyo33e",
        "outputId": "3ad85c88-88b7-43ff-ef85-41286b9d80c5"
      },
      "source": [
        "# Parsing the data into batches using DataLoader\n",
        "data = list(zip(training_corpus, training_labels, training_lengths))\n",
        "loader = DataLoader(data, batch_size = hyperparameters[\"batch_size\"], shuffle = True)\n",
        "\n",
        "# Defining a custom loss function to account for variable length\n",
        "def loss_func(y, preds, length):\n",
        "  loss = nn.BCELoss()\n",
        "  loss_val = loss(preds, y.float())\n",
        "  loss_val = loss_val / length.sum().float()\n",
        "  return loss_val\n",
        "\n",
        "# Initializing the models\n",
        "RNN_model = SentimentRNN(hyperparameters)\n",
        "LSTM_model = SentimentLSTM(hyperparameters)\n",
        "GRU_model = SentimentGRU(hyperparameters)\n",
        "# Initializing an SGD optimizer for each model\n",
        "RNN_SGD = optim.SGD(RNN_model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "LSTM_SGD = optim.SGD(LSTM_model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "GRU_SGD = optim.SGD(GRU_model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "models = [RNN_model, LSTM_model, GRU_model]\n",
        "optimizers = [RNN_SGD, LSTM_SGD, GRU_SGD]\n",
        "# Defining a train function that will train the models over a given number of epochs\n",
        "def train(models, optimizers, loader, nepoch=100):\n",
        "  for i in range(nepoch):\n",
        "    for x_batch, y_batch, length_batch in loader:\n",
        "      optimizers[0].zero_grad()\n",
        "      preds = models[0](x_batch)\n",
        "      RNN_loss = loss_func(y_batch, preds, length_batch)\n",
        "      RNN_loss.backward()\n",
        "      optimizers[0].step()\n",
        "\n",
        "      optimizers[1].zero_grad()\n",
        "      preds = models[1](x_batch)\n",
        "      LSTM_loss = loss_func(y_batch, preds, length_batch)\n",
        "      LSTM_loss.backward()\n",
        "      optimizers[1].step()\n",
        "\n",
        "      optimizers[2].zero_grad()\n",
        "      preds = models[2](x_batch)\n",
        "      GRU_loss = loss_func(y_batch, preds, length_batch)\n",
        "      GRU_loss.backward()\n",
        "      optimizers[2].step()\n",
        "    if i % 10 == 0:\n",
        "      print(f\"Simple RNN Loss: {RNN_loss}\")\n",
        "      print(f\"Simple LSTM Loss: {LSTM_loss}\")\n",
        "      print(f\"Simple GRU Loss: {GRU_loss}\")\n",
        "\n",
        "train(models, optimizers, loader)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple RNN Loss: 0.00016884817159734666\n",
            "Simple LSTM Loss: 0.00016688869800418615\n",
            "Simple GRU Loss: 0.00016816092829685658\n",
            "Simple RNN Loss: 0.000153010492795147\n",
            "Simple LSTM Loss: 0.00015051108493935317\n",
            "Simple GRU Loss: 0.00015171938866842538\n",
            "Simple RNN Loss: 0.00016742543084546924\n",
            "Simple LSTM Loss: 0.00016304533346556127\n",
            "Simple GRU Loss: 0.00016478399629704654\n",
            "Simple RNN Loss: 0.00015183120558504015\n",
            "Simple LSTM Loss: 0.00014834340254310519\n",
            "Simple GRU Loss: 0.00014937165542505682\n",
            "Simple RNN Loss: 0.00014401554653886706\n",
            "Simple LSTM Loss: 0.00014173315139487386\n",
            "Simple GRU Loss: 0.00014277802256401628\n",
            "Simple RNN Loss: 0.0001378691813442856\n",
            "Simple LSTM Loss: 0.00013555561599787325\n",
            "Simple GRU Loss: 0.00013628765009343624\n",
            "Simple RNN Loss: 0.0001554391928948462\n",
            "Simple LSTM Loss: 0.0001529199507785961\n",
            "Simple GRU Loss: 0.0001539181830594316\n",
            "Simple RNN Loss: 0.0001323157048318535\n",
            "Simple LSTM Loss: 0.00013137435598764569\n",
            "Simple GRU Loss: 0.00013175740605220199\n",
            "Simple RNN Loss: 0.00016618764493614435\n",
            "Simple LSTM Loss: 0.0001638765970710665\n",
            "Simple GRU Loss: 0.0001648805191507563\n",
            "Simple RNN Loss: 0.0001618307869648561\n",
            "Simple LSTM Loss: 0.00016029970720410347\n",
            "Simple GRU Loss: 0.00016116281040012836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4MIIi3xk0C",
        "outputId": "ad3da11c-e44e-4351-b673-7e95897f8574"
      },
      "source": [
        "data = list(zip(test_corpus, test_labels, test_lengths))\n",
        "loader = DataLoader(data, batch_size = 256, shuffle = True)\n",
        "\n",
        "def test(models, loader):\n",
        "  counter = 0\n",
        "  total_rnn_loss = 0\n",
        "  total_lstm_loss = 0\n",
        "  total_gru_loss = 0\n",
        "  for x_batch, y_batch, length_batch in loader:\n",
        "    preds = models[0](x_batch)\n",
        "    total_rnn_loss += loss_func(y_batch, preds, length_batch)\n",
        "    preds = models[1](x_batch)\n",
        "    total_lstm_loss += loss_func(y_batch, preds, length_batch)\n",
        "    preds = models[2](x_batch)\n",
        "    total_gru_loss += loss_func(y_batch, preds, length_batch)\n",
        "    counter += 1\n",
        "  losses = [total_rnn_loss.item()/counter, total_lstm_loss.item()/counter, total_gru_loss.item()/counter]\n",
        "  return losses\n",
        "\n",
        "losses = test(models, loader)\n",
        "print(f\"Final Loss of the simple RNN model on the Test Set: {losses[0]}\")\n",
        "print(f\"Final Loss of the LSTM model on the Test Set: {losses[1]}\")\n",
        "print(f\"Final Loss of the GRU model on the Test Set: {losses[2]}\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Loss of the simple RNN model on the Test Set: 2.314013363502454e-05\n",
            "Final Loss of the LSTM model on the Test Set: 2.2718817490385845e-05\n",
            "Final Loss of the GRU model on the Test Set: 2.2915784938959405e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}